return(score)
}, pos.words, neg.words )
Qscores.df = data.frame(score=scores, text=Queens)
return(scores.df)
}
Steuben <- searchTwitter('Donald Trump', geocode='42.329592,-77.6594314,20mi', n = 2500)
if (!require(twitteR)) {install.packages("twitteR")}
if (!require(ROAuth)) {install.packages("ROAuth")}
if (!require(plyr)) {install.packages("plyr")}
if (!require(dplyr)) {install.packages("dplyr")}
if (!require(stringr)) {install.packages("stringr")}
if (!require(ggplot2)) {install.packages("ggplot2")}
library(twitteR)
library(ROAuth)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
my.consumer.key = "duQPUERPEWDUzMrsfeBoCKFEt"
my.consumer.secret = "aBjZ56UY9JPqUC0lEUJxMn20p0Z4r62XpAWsKmXyeMVWt6cDJb"
my.access.token = "2189150074-g1RNhU5qbJl6sCHU7h2ZDmZKDYEUWdMhvTiLMVY"
my.access.token.secret = "d94WyPVX4rbKlrxjlJu6QS7bRpdbQcUH0LR5z3KX78bdh"
my_oauth <- setup_twitter_oauth(consumer_key = my.consumer.key, consumer_secret = my.consumer.secret, access_token = my.access.token, access_secret = my.access.token.secret)
save(my_oauth, file = "my_oauth.Rdata")
neg = scan("GEOG533/Lectures/Lecture_W08_Twitter_Analytics/negative-words.txt", what="character", comment.char=";")
pos = scan("GEOG533/Lectures/Lecture_W08_Twitter_Analytics/positive-words.txt", what="character", comment.char=";")
score.sentiment = function(Queens, pos.words, neg.words)
{
scores = laply(Queens, function(Queens, pos.words, neg.words) {
Queens = gsub('https://','',Queens)
Queens = gsub('http://','',Queens)
Queens = gsub('[^[:graph:]]', ' ',Queens)
Queens = gsub('[[:punct:]]', '', Queens)
Queens = gsub('[[:cntrl:]]', '', Queens)
Queens = gsub('\\d+', '', Queens)
Queens = tolower(Queens)
word.list = str_split(Queens, '\\s+')
words = unlist(word.list)
pos.matches =  match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words )
Qscores.df = data.frame(score=scores, text=Queens)
return(scores.df)
}
Steuben <- searchTwitter('Donald Trump', geocode='42.329592,-77.6594314,20mi', n = 2500)
Steuben <- searchTwitter('Donald Trump', geocode='42.329592,-77.6594314,20mi', n = 200)
Steuben <- searchTwitter('Donald Trump', geocode='42.329592,-77.6594314,20mi', n = 100)
Tweets.text = laply(tweets,function(t)t$getText())
Tweets.text = laply(Queens,function(t)t$getText())
analysis = score.sentiment(Tweets.text, pos, neg)
score.sentiment = function(Queens, pos.words, neg.words)
{
scores = laply(Queens, function(Queens, pos.words, neg.words) {
Queens = gsub('https://','',Queens)
Queens = gsub('http://','',Queens)
Queens = gsub('[^[:graph:]]', ' ',Queens)
Queens = gsub('[[:punct:]]', '', Queens)
Queens = gsub('[[:cntrl:]]', '', Queens)
Queens = gsub('\\d+', '', Queens)
Queens = tolower(Queens)
word.list = str_split(Queens, '\\s+')
words = unlist(word.list)
pos.matches =  match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words )
Qscores.df = data.frame(score=scores, text=Queens)
return(scores.df)
}
Tweets.text= laply(Queens,function(t)t$getText())
analysis = score.sentiment(Tweets.text, pos, neg)
return(Steuben_df)
score.sentiment = function(Queens, pos.words, neg.words)
{
scores = laply(Queens, function(Queens, pos.words, neg.words) {
Queens = gsub('https://','',Queens)
Queens = gsub('http://','',Queens)
Queens = gsub('[^[:graph:]]', ' ',Queens)
Queens = gsub('[[:punct:]]', '', Queens)
Queens = gsub('[[:cntrl:]]', '', Queens)
Queens = gsub('\\d+', '', Queens)
Queens = tolower(Queens)
word.list = str_split(Queens, '\\s+')
words = unlist(word.list)
pos.matches =  match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words )
Qscores.df = data.frame(score=scores, text=Queens)
return(Steuben_df)
}
Tweets.text= laply(Queens,function(t)t$getText())
analysis = score.sentiment(Tweets.text, pos, neg)
score.sentiment = function(Queens, pos.words, neg.words)
{
scores = laply(Queens, function(Queens, pos.words, neg.words) {
Queens = gsub('https://','',Queens)
Queens = gsub('http://','',Queens)
Queens = gsub('[^[:graph:]]', ' ',Queens)
Queens = gsub('[[:punct:]]', '', Queens)
Queens = gsub('[[:cntrl:]]', '', Queens)
Queens = gsub('\\d+', '', Queens)
Queens = tolower(Queens)
word.list = str_split(Queens, '\\s+')
words = unlist(word.list)
pos.matches =  match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words )
Qscores.df = data.frame(score=scores, text=Queens)
return(Queens_df)
}
Tweets.text= laply(Queens,function(t)t$getText())
analysis = score.sentiment(Tweets.text, pos, neg)
View(analysis)
score.sentiment = function(tweets, pos.words, neg.words, .progress = "none")
{
scores = laply(tweets, function(tweet, pos.words, neg.words) {
tweet = gsub('https://','',tweet)
tweet = gsub('http://','',tweet)
tweet = gsub('[^[:graph:]]', ' ',tweet)
tweet = gsub('[[:punct:]]', '', tweet)
tweet = gsub('[[:cntrl:]]', '', tweet)
tweet = gsub('\\d+', '', tweet)
tweet = tolower(tweet)
word.list = str_split(tweet, '\\s+')
words = unlist(word.list)
pos.matches =  match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words )
Qscores.df = data.frame(score=scores, text=Queens)
return(Queens_df)
}
score.sentiment = function(tweets, pos.words, neg.words, .progress = "none")
{
scores = laply(tweets, function(tweet, pos.words, neg.words) {
tweet = gsub('https://','',tweet)
tweet = gsub('http://','',tweet)
tweet = gsub('[^[:graph:]]', ' ',tweet)
tweet = gsub('[[:punct:]]', '', tweet)
tweet = gsub('[[:cntrl:]]', '', tweet)
tweet = gsub('\\d+', '', tweet)
tweet = tolower(tweet)
word.list = str_split(tweet, '\\s+')
words = unlist(word.list)
pos.matches =  match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words )
Qscores.df = data.frame(score=scores, text=tweets)
return(Queens_df)
}
sum(tweet(text))
sum(score)
qscore <- score.sentiment(Queens_df$text, pos.words, neg.words, .progress = text)
score.sentiment = function(tweets, pos.words, neg.words, .progress = "none")
{
scores = laply(tweets, function(tweet, pos.words, neg.words) {
tweet = gsub('https://','',tweet)
tweet = gsub('http://','',tweet)
tweet = gsub('[^[:graph:]]', ' ',tweet)
tweet = gsub('[[:punct:]]', '', tweet)
tweet = gsub('[[:cntrl:]]', '', tweet)
tweet = gsub('\\d+', '', tweet)
tweet = tolower(tweet)
word.list = str_split(tweet, '\\s+')
words = unlist(word.list)
pos.matches =  match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words )
Qscores.df = data.frame(score=scores, text=tweets)
return(Queens_df)
}
qscore <- score.sentiment(Queens_df$text, pos.words, neg.words, .progress = text)
qscore <- score.sentiment(Queens_df$text)
qscore <- score.sentiment(Queens_df$text), function(tweets, pos.words, neg.words, .progress = "none")
qscore <- score.sentiment(Queens_df$text, function(tweets, pos.words, neg.words, .progress = "none")
qscore <- score.sentiment(Queens_df$text, function(tweets, pos.words, neg.words, .progress = "none"))
qscore <- score.sentiment(Queens_df$text, function(tweets, pos.words, neg.words, .progress = "text"))
qscore <- score.sentiment(Queens_df$text, function(tweets, pos.words, neg.words, .progress = "text")
score.sentiment = function(tweets, pos.words, neg.words)
{
scores = laply(tweets, function(tweet, pos.words, neg.words) {
tweet = gsub('https://','',tweet)
tweet = gsub('http://','',tweet)
tweet=gsub('[^[:graph:]]', ' ',tweet)
tweet = gsub('[[:punct:]]', '', tweet)
tweet = gsub('[[:cntrl:]]', '', tweet)
tweet = gsub('\\d+', '', tweet)
tweet=str_replace_all(tweet,"[^[:graph:]]", " ")
tweet = tolower(tweet)
word.list = str_split(tweet, '\\s+')
words = unlist(word.list)
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words )
scores.df = data.frame(score=scores, text=tweets)
return(scores.df)
}
if (!require(twitteR)) {install.packages("twitteR")}
if (!require(ROAuth)) {install.packages("ROAuth")}
if (!require(plyr)) {install.packages("plyr")}
if (!require(dplyr)) {install.packages("dplyr")}
if (!require(stringr)) {install.packages("stringr")}
if (!require(ggplot2)) {install.packages("ggplot2")}
library(twitteR)
library(ROAuth)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
my.consumer.key = "duQPUERPEWDUzMrsfeBoCKFEt"
my.consumer.secret = "aBjZ56UY9JPqUC0lEUJxMn20p0Z4r62XpAWsKmXyeMVWt6cDJb"
my.access.token = "2189150074-g1RNhU5qbJl6sCHU7h2ZDmZKDYEUWdMhvTiLMVY"
my.access.token.secret = "d94WyPVX4rbKlrxjlJu6QS7bRpdbQcUH0LR5z3KX78bdh"
my_oauth <- setup_twitter_oauth(consumer_key = my.consumer.key, consumer_secret = my.consumer.secret, access_token = my.access.token, access_secret = my.access.token.secret)
save(my_oauth, file = "my_oauth.Rdata")
neg = scan("GEOG533/Lectures/Lecture_W08_Twitter_Analytics/negative-words.txt", what="character", comment.char=";")
pos = scan("GEOG533/Lectures/Lecture_W08_Twitter_Analytics/positive-words.txt", what="character", comment.char=";")
score.sentiment = function(tweets, pos.words, neg.words)
{
scores = laply(tweets, function(tweet, pos.words, neg.words) {
tweet = gsub('https://','',tweet)
tweet = gsub('http://','',tweet)
tweet=gsub('[^[:graph:]]', ' ',tweet)
tweet = gsub('[[:punct:]]', '', tweet)
tweet = gsub('[[:cntrl:]]', '', tweet)
tweet = gsub('\\d+', '', tweet)
tweet=str_replace_all(tweet,"[^[:graph:]]", " ")
tweet = tolower(tweet)
word.list = str_split(tweet, '\\s+')
words = unlist(word.list)
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words )
scores.df = data.frame(score=scores, text=tweets)
return(scores.df)
}
tweets = searchTwitter('Donald Trump', geocode='42.329592,-77.6594314,20mi', n = 100)
Tweets.text = laply(tweets,function(t)t$getText())
analysis = score.sentiment(Tweets.text, pos, neg)
View(analysis)
if (!require(twitteR)) {install.packages("twitteR")}
if (!require(ROAuth)) {install.packages("ROAuth")}
if (!require(plyr)) {install.packages("plyr")}
if (!require(dplyr)) {install.packages("dplyr")}
if (!require(stringr)) {install.packages("stringr")}
if (!require(ggplot2)) {install.packages("ggplot2")}
library(twitteR)
library(ROAuth)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
my.consumer.key = "duQPUERPEWDUzMrsfeBoCKFEt"
my.consumer.secret = "aBjZ56UY9JPqUC0lEUJxMn20p0Z4r62XpAWsKmXyeMVWt6cDJb"
my.access.token = "2189150074-g1RNhU5qbJl6sCHU7h2ZDmZKDYEUWdMhvTiLMVY"
my.access.token.secret = "d94WyPVX4rbKlrxjlJu6QS7bRpdbQcUH0LR5z3KX78bdh"
my_oauth <- setup_twitter_oauth(consumer_key = my.consumer.key, consumer_secret = my.consumer.secret, access_token = my.access.token, access_secret = my.access.token.secret)
save(my_oauth, file = "my_oauth.Rdata")
neg = scan("GEOG533/Lectures/Lecture_W08_Twitter_Analytics/negative-words.txt", what="character", comment.char=";")
pos = scan("GEOG533/Lectures/Lecture_W08_Twitter_Analytics/positive-words.txt", what="character", comment.char=";")
score.sentiment = function(tweets, pos.words, neg.words)
{
scores = laply(tweets, function(tweet, pos.words, neg.words) {
tweet = gsub('https://','',tweet)
tweet = gsub('http://','',tweet)
tweet=gsub('[^[:graph:]]', ' ',tweet)
tweet = gsub('[[:punct:]]', '', tweet)
tweet = gsub('[[:cntrl:]]', '', tweet)
tweet = gsub('\\d+', '', tweet)
tweet=str_replace_all(tweet,"[^[:graph:]]", " ")
tweet = tolower(tweet)
word.list = str_split(tweet, '\\s+')
words = unlist(word.list)
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words )
scores.df = data.frame(score=scores, text=tweets)
return(scores.df)
}
#Steuben
tweets = searchTwitter('Donald Trump', geocode='42.329592,-77.6594314,20mi', n = 100)
Tweets.text = laply(tweets,function(t)t$getText())
SCanalysis = score.sentiment(Tweets.text, pos, neg)
#Broome
tweets = searchTwitter('Donald Trump', geocode='29.8174,-95.6814,20mi',  n = 100)
Tweets.text = laply(tweets,function(t)t$getText())
BCanalysis = score.sentiment(Tweets.text, pos, neg)
#queens
tweets = searchTwitter('Donald Trump', geocode='40.660787,-73.738861,20mi',  n = 100)
Tweets.text = laply(tweets,function(t)t$getText())
QCanalysis = score.sentiment(Tweets.text, pos, neg)
#Dutchess
tweets = searchTwitter('Donald Trump', geocode='41.713453,-73.798813,20mi',  n = 100)
Tweets.text = laply(tweets,function(t)t$getText())
DCanalysis = score.sentiment(Tweets.text, pos, neg)
View(BCanalysis)
View(Broome_df)
View(DCanalysis)
View(QCanalysis)
View(SCanalysis)
sum(SCanalysis$score)
sum(BCanalysis$score)
sum(QCanalysis$score)
sum(DCanalysis$score)
#Dutchess
tweets = searchTwitter('Trump', geocode='41.713453,-73.798813,20mi',  n = 100)
Tweets.text = laply(tweets,function(t)t$getText())
DCanalysis = score.sentiment(Tweets.text, pos, neg)
sum(DCanalysis$score)
#queens
tweets = searchTwitter('Trump', geocode='40.660787,-73.738861,20mi',  n = 100)
Tweets.text = laply(tweets,function(t)t$getText())
QCanalysis = score.sentiment(Tweets.text, pos, neg)
sum(QCanalysis$score)
#Broome
tweets = searchTwitter('Trump', geocode='29.8174,-95.6814,20mi',  n = 100)
Tweets.text = laply(tweets,function(t)t$getText())
BCanalysis = score.sentiment(Tweets.text, pos, neg)
sum(BCanalysis$score)
tweets = searchTwitter('Trump', geocode='42.329592,-77.6594314,20mi', n = 100)
#Steuben
tweets = searchTwitter('Trump', geocode='42.329592,-77.6594314,20mi', n = 100)
Tweets.text = laply(tweets,function(t)t$getText())
SCanalysis = score.sentiment(Tweets.text, pos, neg)
sum(SCanalysis$score)
tweets.text <- sapply(tweets, function(x) x$getText())
tweets.text <- gsub("rt", "", tweets.text)
# Replace @UserName
tweets.text <- gsub("@\\w+", "", tweets.text)
# Remove punctuation
tweets.text <- gsub("[[:punct:]]", "", tweets.text)
# Remove links
tweets.text <- gsub("http\\w+", "", tweets.text)
# Remove tabs
tweets.text <- gsub("[ |\t]{2,}", "", tweets.text)
# Remove blank spaces at the beginning
tweets.text <- gsub("^ ", "", tweets.text)
# Remove blank spaces at the end
tweets.text <- gsub(" $", "", tweets.text)
# #convert all text to lower case
tweets.text <- tolower(tweets.text)
if(!require(tm)) {install.packages("tm")}
library(tm)
#create corpus
tweets.text.corpus <- Corpus(VectorSource(tweets.text))
#clean up by removing stop words
tweets.text.corpus <- tm_map(tweets.text.corpus, function(x) removeWords(x,stopwords()))
if(!require(wordcloud)) {install.packages("wordcloud")}
library(wordcloud)
#generate wordcloud
wordcloud(tweets.text.corpus,min.freq = 2, scale=c(7,0.5),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 150)
tweets.text <- sapply(tweets, function(x) x$getText())
tweets.text <- gsub("rt", "", tweets.text)
# Replace @UserName
tweets.text <- gsub("@\\w+", "", tweets.text)
# Remove punctuation
tweets.text <- gsub("[[:punct:]]", "", tweets.text)
# Remove links
tweets.text <- gsub("http\\w+", "", tweets.text)
# Remove tabs
tweets.text <- gsub("[ |\t]{2,}", "", tweets.text)
# Remove blank spaces at the beginning
tweets.text <- gsub("^ ", "", tweets.text)
# Remove blank spaces at the end
tweets.text <- gsub(" $", "", tweets.text)
# #convert all text to lower case
tweets.text <- tolower(tweets.text)
if(!require(tm)) {install.packages("tm")}
library(tm)
#create corpus
tweets.text.corpus <- Corpus(VectorSource(tweets.text))
#clean up by removing stop words
tweets.text.corpus <- tm_map(tweets.text.corpus, function(x) removeWords(x,stopwords()))
if(!require(wordcloud)) {install.packages("wordcloud")}
library(wordcloud)
#generate wordcloud
wordcloud(tweets.text.corpus,min.freq = 2, scale=c(7,0.5),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 150)
if (!require(twitteR)) {install.packages("twitteR")}
if (!require(ROAuth)) {install.packages("ROAuth")}
if (!require(plyr)) {install.packages("plyr")}
if (!require(dplyr)) {install.packages("dplyr")}
if (!require(stringr)) {install.packages("stringr")}
if (!require(ggplot2)) {install.packages("ggplot2")}
library(twitteR)
library(ROAuth)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
my.consumer.key = "duQPUERPEWDUzMrsfeBoCKFEt"
my.consumer.secret = "aBjZ56UY9JPqUC0lEUJxMn20p0Z4r62XpAWsKmXyeMVWt6cDJb"
my.access.token = "2189150074-g1RNhU5qbJl6sCHU7h2ZDmZKDYEUWdMhvTiLMVY"
my.access.token.secret = "d94WyPVX4rbKlrxjlJu6QS7bRpdbQcUH0LR5z3KX78bdh"
my_oauth <- setup_twitter_oauth(consumer_key = my.consumer.key, consumer_secret = my.consumer.secret, access_token = my.access.token, access_secret = my.access.token.secret)
save(my_oauth, file = "my_oauth.Rdata")
neg = scan("GEOG533/Lectures/Lecture_W08_Twitter_Analytics/negative-words.txt", what="character", comment.char=";")
pos = scan("GEOG533/Lectures/Lecture_W08_Twitter_Analytics/positive-words.txt", what="character", comment.char=";")
score.sentiment = function(tweets, pos.words, neg.words)
{
scores = laply(tweets, function(tweet, pos.words, neg.words) {
tweet = gsub('https://','',tweet)
tweet = gsub('http://','',tweet)
tweet=gsub('[^[:graph:]]', ' ',tweet)
tweet = gsub('[[:punct:]]', '', tweet)
tweet = gsub('[[:cntrl:]]', '', tweet)
tweet = gsub('\\d+', '', tweet)
tweet=str_replace_all(tweet,"[^[:graph:]]", " ")
tweet = tolower(tweet)
word.list = str_split(tweet, '\\s+')
words = unlist(word.list)
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words )
scores.df = data.frame(score=scores, text=tweets)
return(scores.df)
}
#Steuben
tweets = searchTwitter('Donand Trump', geocode='42.329592,-77.6594314,20mi', n = 100)
Tweets.text = laply(tweets,function(t)t$getText())
SCanalysis = score.sentiment(Tweets.text, pos, neg)
sum(SCanalysis$score)
tweets.text <- sapply(tweets, function(x) x$getText())
tweets.text <- gsub("rt", "", tweets.text)
tweets.text <- gsub("@\\w+", "", tweets.text)
tweets.text <- gsub("[[:punct:]]", "", tweets.text)
tweets.text <- gsub("http\\w+", "", tweets.text)
tweets.text <- gsub("[ |\t]{2,}", "", tweets.text)
tweets.text <- gsub("^ ", "", tweets.text)
tweets.text <- gsub(" $", "", tweets.text)
tweets.text <- tolower(tweets.text)
if(!require(tm)) {install.packages("tm")}
library(tm)
tweets.text.corpus <- Corpus(VectorSource(tweets.text))
tweets.text.corpus <- tm_map(tweets.text.corpus, function(x) removeWords(x,stopwords()))
if(!require(wordcloud)) {install.packages("wordcloud")}
library(wordcloud)
wordcloud(tweets.text.corpus,min.freq = 2, scale=c(7,0.5),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 150)
score.sentiment = function(tweets, pos.words, neg.words)
{
scores = laply(tweets, function(tweet, pos.words, neg.words) {
tweet = gsub('https://','',tweet)
tweet = gsub('http://','',tweet)
tweet=gsub('[^[:graph:]]', ' ',tweet)
tweet = gsub('[[:punct:]]', '', tweet)
tweet = gsub('[[:cntrl:]]', '', tweet)
tweet = gsub('\\d+', '', tweet)
tweet=str_replace_all(tweet,"[^[:graph:]]", " ")
tweet = tolower(tweet)
word.list = str_split(tweet, '\\s+')
words = unlist(word.list)
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words )
scores.df = data.frame(score=scores, text=tweets)
return(scores.df)
}
#Steuben
tweets = searchTwitter('Donand Trump', geocode='42.329592,-77.6594314,20mi', n = 100)
Tweets.text = laply(tweets,function(t)t$getText())
SCanalysis = score.sentiment(Tweets.text, pos, neg)
sum(SCanalysis$score)
#Steuben
tweets = searchTwitter('Donand Trump', geocode='42.329592,-77.6594314,20mi', n = 100)
Tweets.text = laply(tweets,function(t)t$getText())
SCanalysis = score.sentiment(Tweets.text, pos, neg)
sum(SCanalysis$score)
